{"cells":[{"cell_type":"markdown","metadata":{"id":"M55XWPEiVglM"},"source":["### Setting ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15632,"status":"ok","timestamp":1700452454410,"user":{"displayName":"문경","userId":"03211429860722808255"},"user_tz":-540},"id":"MrPgiVdCxqET","outputId":"53571a54-0dc8-46b9-9f47-03a3eed3f65d"},"outputs":[],"source":["# Google Drive Mount\n","#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8576,"status":"ok","timestamp":1700452477626,"user":{"displayName":"문경","userId":"03211429860722808255"},"user_tz":-540},"id":"_LO5c6SK5j4t","outputId":"a32aefb6-b05e-4f2e-b803-2802caedaf7d"},"outputs":[],"source":["# Install foundation model - Segment Anything\n","#!pip install git+https://github.com/facebookresearch/segment-anything.git"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10845,"status":"ok","timestamp":1700452490365,"user":{"displayName":"문경","userId":"03211429860722808255"},"user_tz":-540},"id":"bmvdJPVdqC_8"},"outputs":[],"source":["# libraries\n","import os\n","import glob\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","import tifffile as tiff\n","import cv2\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as tf\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm.notebook import tqdm\n","\n","from transformers import SamProcessor, SamModel\n","import monai"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# configuration\n","batch_size = 3\n","epochs = 500\n","lr = 0.00005\n","weight_decay = 0\n","\n","device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(device)\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# directory setting\n","class ROOTDIR:\n","    image = \"/home/kmk/COSE474Project/data/images/\"\n","    mask = \"/home/kmk/COSE474Project/data/masks/\""]},{"cell_type":"markdown","metadata":{},"source":["### Data example ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["images = sorted(glob.glob(ROOTDIR.image + \"*.tif\"))\n","masks = sorted(glob.glob(ROOTDIR.mask + \"*.tif\"))\n","fns = sorted([i.split(\"/\")[-1].split(\".\")[0] for i in images])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(images), len(masks), len(fns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train/val/test split (8:1:1)\n","train_images, val_images, train_masks, val_masks, train_fns, val_fns = train_test_split(images, masks, fns, test_size=0.2)\n","val_images, test_images, val_masks, test_masks, val_fns, test_fns = train_test_split(val_images, val_masks, val_fns, test_size=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(train_images), len(val_images), len(test_images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_bbox(gt_mask):\n","    y_indices, x_indices = np.where(gt_mask > 0)\n","    x_min, x_max = np.min(x_indices), np.max(x_indices)\n","    y_min, y_max = np.min(y_indices), np.max(y_indices)\n","    \n","    h, w = gt_mask.shape\n","    x_min = max(0, x_min-np.random.randint(0, 10))\n","    x_max = min(w, x_max+np.random.randint(0, 10))\n","    y_min = max(0, y_min-np.random.randint(0, 10))\n","    y_max = min(h, y_max+np.random.randint(0, 10))\n","    \n","    bbox = [x_min, y_min, x_max, y_max]\n","    \n","    return bbox"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_bbox(bbox):\n","    ax = plt.gca()\n","    \n","    w = bbox[2] - bbox[0]\n","    h = bbox[3] - bbox[1]\n","        \n","    rect = patches.Rectangle((bbox[0], bbox[1]), w, h, color=\"blue\", fill=False)\n","        \n","    ax.add_patch(rect)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_mask_on_image(mask):\n","    color = np.array([0, 255, 0, 0.6])\n","    \n","    if len(mask.shape) == 4:\n","        mask = mask.squeeze()\n","    \n","    h, w = mask.shape[-2:]\n","    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n","    ax = plt.gca()\n","    ax.imshow(mask_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"elapsed":2165,"status":"ok","timestamp":1700208668361,"user":{"displayName":"문경","userId":"03211429860722808255"},"user_tz":-540},"id":"KXFDGRnTqDAC","outputId":"611669fb-d684-4d59-c7dd-d513878ba2d3"},"outputs":[],"source":["ex_img = tiff.imread(train_images[0])\n","ex_img = np.array(ex_img)\n","ex_mask = cv2.imread(train_masks[0], cv2.IMREAD_UNCHANGED)\n","ex_mask = np.array(ex_mask)\n","ex_bbox = get_bbox(ex_mask)\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(ex_img)\n","plt.title(\"image\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(ex_img)\n","show_bbox(ex_bbox)\n","plt.title(\"bounding box\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 3, 3)\n","plt.imshow(ex_mask)\n","plt.title(\"ground truth mask\")\n","plt.axis(\"off\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Zero shot prediction ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n","processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputs = processor(ex_img, input_boxes=[[[ex_bbox]]], return_tensors=\"pt\").to(device)\n","\n","with torch.no_grad():\n","    outputs = model(**inputs, multimask_output=False)\n","    \n","masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), \n","                                                     inputs[\"original_sizes\"].cpu(),\n","                                                     inputs[\"reshaped_input_sizes\"].cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.subplot(1, 2, 1)\n","plt.imshow(ex_img)\n","show_mask_on_image(masks[0])\n","plt.title(\"predicted mask\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(ex_mask)\n","plt.title(\"ground truth mask\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qsArht2kTR1Q"},"source":["### Prepare Dataset ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MedDataset(Dataset):\n","    def __init__(self, img_dir, mask_dir, processor, mode):\n","        self.img_dir = img_dir\n","        self.mask_dir = mask_dir\n","        self.processor = processor\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.img_dir)\n","    \n","    def __getitem__(self, idx):\n","        image_dir = self.img_dir[idx]\n","        mask_dir = self.mask_dir[idx]\n","        \n","        image = tiff.imread(image_dir)\n","        image = np.array(image)\n","        \n","        mask = tiff.imread(mask_dir)\n","        mask = np.array(mask)\n","        gt_mask = (cv2.imread(mask_dir, cv2.IMREAD_GRAYSCALE) / 255.).astype(np.uint8)\n","        \n","        bbox = get_bbox(np.array(gt_mask))\n","        \n","        inputs = self.processor(image, input_boxes=[[bbox]], return_tensors=\"pt\")\n","        \n","        inputs = {k:v.squeeze(0) for k, v in inputs.items()}\n","        \n","        gt_mask = cv2.resize(gt_mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n","        \n","        inputs[\"ground_truth_mask\"] = gt_mask\n","        \n","        if self.mode == \"test\":\n","            return image, mask, bbox, inputs\n","        else:\n","            return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = MedDataset(img_dir=train_images[:30], mask_dir=train_masks[:30], processor=processor, mode=\"train\")\n","val_data = MedDataset(img_dir=val_images[:30], mask_dir=val_masks[:30], processor=processor, mode=\"val\")\n","\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch = next(iter(train_dataloader))\n","for k, v in batch.items():\n","    print(k, v.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Training ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tensorboard\n","writer = SummaryWriter()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, dataloader, optimizer, criterion, epoch):\n","    model.train()\n","    train_running_loss = 0.0\n","    \n","    for j, batch in enumerate(tqdm(dataloader)):\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        input_boxes = batch[\"input_boxes\"].to(device)\n","        gt_masks = batch[\"ground_truth_mask\"].float().to(device)\n","        \n","        outputs = model(pixel_values=pixel_values, input_boxes=input_boxes, multimask_output=False)\n","        \n","        predicted_masks = torch.sigmoid(outputs.pred_masks.squeeze(1)).to(device)\n","        \n","        loss = criterion(predicted_masks, gt_masks.unsqueeze(1))\n","        \n","        writer.add_scalar(\"Loss/train\", loss, j+epoch*len(dataloader))\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        \n","        optimizer.step()       \n","        \n","        train_running_loss += loss.item()\n","    \n","    train_loss = train_running_loss / (j+1)\n","    \n","    return train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def val_model(model, dataloader, criterion, epoch):\n","    model.eval()\n","    val_running_loss = 0.0\n","    \n","    with torch.no_grad():\n","        for j, batch in enumerate(tqdm(dataloader)):\n","            pixel_values = batch[\"pixel_values\"].to(device)\n","            input_boxes = batch[\"input_boxes\"].to(device)\n","            gt_masks = batch[\"ground_truth_mask\"].float().to(device)\n","            \n","            outputs = model(pixel_values=pixel_values, input_boxes=input_boxes, multimask_output=False)\n","            \n","            predicted_masks = torch.sigmoid(outputs.pred_masks.squeeze(1)).to(device)\n","            \n","            loss = criterion(predicted_masks, gt_masks.unsqueeze(1))\n","            \n","            writer.add_scalar(\"Loss/validation\", loss, j+epoch*len(dataloader))\n","            \n","            val_running_loss += loss.item()\n","            \n","        val_loss = val_running_loss / (j+1)\n","        \n","        return val_loss, model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=20, verbose=False, delta=0, trace_func=print):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.delta = delta\n","        self.trace_func = trace_func\n","        self.counter = 0\n","        self.val_loss = None\n","        self.val_loss_min = np.Inf\n","        self.early_stop = False\n","        \n","    def __call__(self, val_loss, model, file_name):\n","        if self.val_loss is None:\n","            self.val_loss = val_loss\n","            self.save_checkpoint(val_loss, model, file_name)\n","        elif val_loss > self.val_loss + self.delta:\n","            self.counter += 1\n","            self.trace_func(f\"Early Stopping counter: {self.counter} out of {self.patience}\")\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.val_loss = val_loss\n","            self.save_checkpoint(val_loss, model, file_name)\n","            self.counter = 0\n","            \n","    def save_checkpoint(self, val_loss, model, file_name):\n","        if self.verbose:\n","            self.trace_func(f\"Validation loss decreased: {self.val_loss_min:.6f} --> {val_loss:.6f}. Saving model...\")\n","        torch.save(model.state_dict(), file_name)\n","        self.val_loss_min = val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for name, param in model.named_parameters():\n","    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n","        param.requires_grad_(False)\n","        \n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = optim.Adam(model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch:0.95**epoch)\n","criterion = monai.losses.DiceFocalLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","es = EarlyStopping(patience=10, verbose=False, delta=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loss_list = []\n","val_loss_list = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in tqdm(range(epochs)):\n","    train_loss = train_model(model, train_dataloader, optimizer, criterion, epoch)\n","    val_loss, model = val_model(model, val_dataloader, criterion, epoch)\n","    \n","    train_loss_list.append(train_loss)\n","    val_loss_list.append(val_loss)\n","    \n","    print(f\"Train Loss: {train_loss:.4f}\")\n","    print(f\"Validation Loss: {val_loss:.4f}\")\n","    \n","    TUNED_FILE = f\"/home/kmk/COSE474Project/checkpoint/fine_tuned_sam.pth\"\n","    \n","    '''es(val_loss, model, TUNED_FILE)\n","    \n","    if es.early_stop:\n","        writer.close()\n","        break'''\n","        \n","torch.save(model.state_dict(), TUNED_FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(train_loss_list, label='train loss')\n","plt.plot(val_loss_list, label='val loss')\n","plt.legend(loc=\"upper right\")\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Test ###"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data = MedDataset(test_images, test_masks, processor, \"test\")\n","test_dataloader = DataLoader(test_data, 1, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tuned_model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n","tuned_model.load_state_dict(torch.load(\"/home/kmk/COSE474Project/checkpoint/fine_tuned_sam.pth\"))\n","tuned_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_losses = 0.0\n","\n","for j, (img, mask, bbox, batch) in enumerate(tqdm(test_dataloader)):\n","    pixel_values = batch[\"pixel_values\"].to(device)\n","    input_boxes = batch[\"input_boxes\"].to(device)\n","    gt_masks = batch[\"ground_truth_mask\"].float().to(device)\n","    \n","    outputs = tuned_model(pixel_values=pixel_values, input_boxes=input_boxes, multimask_output=False)\n","    \n","    predicted_masks = torch.sigmoid(outputs.pred_masks.squeeze(1))\n","    \n","    loss = criterion(predicted_masks, gt_masks.unsqueeze(1))\n","    \n","    test_losses += loss.item()\n","    \n","    if (j+1)%10 == 0:\n","        print(f\"{j+1}th data\")\n","\n","        img = img.squeeze().cpu().detach().numpy()\n","        mask = mask.squeeze().cpu().detach().numpy()\n","        seg = processor.image_processor.post_process_masks(\n","            outputs.pred_masks.cpu(),\n","            batch[\"original_sizes\"].cpu(),\n","            batch[\"reshaped_input_sizes\"].cpu(),\n","        )\n","        seg = seg[0].squeeze()\n","        \n","        bbox = list(map(int, bbox))\n","        \n","        plt.figure(figsize=(15, 15))\n","        \n","        plt.subplot(1, 4, 1)\n","        plt.imshow(img, cmap='gray')\n","        plt.title('input image')\n","        plt.axis('off')\n","        \n","        plt.subplot(1, 4, 2)\n","        plt.imshow(img, cmap='gray')\n","        show_bbox(bbox)\n","        plt.title('prompt')\n","        plt.axis('off')\n","        \n","        plt.subplot(1, 4, 3)\n","        plt.imshow(img, cmap='gray')\n","        show_mask_on_image(seg)\n","        plt.title('predicted mask')\n","        plt.axis('off')\n","        \n","        plt.subplot(1, 4, 4)\n","        plt.imshow(mask, cmap='gray')\n","        plt.title('ground truth mask')\n","        plt.axis('off')\n","        \n","        plt.show()\n","        \n","test_loss = test_losses / (j+1)\n","\n","print(f\"Test Loss: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["IAbVSYg2ecdX"],"machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":0}
